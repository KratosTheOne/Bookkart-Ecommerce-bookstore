{"ast":null,"code":"import { __extends } from \"tslib\";\nimport { CreateDeliveryStreamInput, CreateDeliveryStreamOutput } from \"../models/models_0\";\nimport { deserializeAws_json1_1CreateDeliveryStreamCommand, serializeAws_json1_1CreateDeliveryStreamCommand } from \"../protocols/Aws_json1_1\";\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\n/**\n * <p>Creates a Kinesis Data Firehose delivery stream.</p>\n *\n *          <p>By default, you can create up to 50 delivery streams per AWS Region.</p>\n *          <p>This is an asynchronous operation that immediately returns. The initial status of the\n *          delivery stream is <code>CREATING</code>. After the delivery stream is created, its status\n *          is <code>ACTIVE</code> and it now accepts data. If the delivery stream creation fails, the\n *          status transitions to <code>CREATING_FAILED</code>. Attempts to send data to a delivery\n *          stream that is not in the <code>ACTIVE</code> state cause an exception. To check the state\n *          of a delivery stream, use <a>DescribeDeliveryStream</a>.</p>\n *          <p>If the status of a delivery stream is <code>CREATING_FAILED</code>, this status\n *          doesn't change, and you can't invoke <code>CreateDeliveryStream</code> again on it.\n *          However, you can invoke the <a>DeleteDeliveryStream</a> operation to delete\n *          it.</p>\n *          <p>A Kinesis Data Firehose delivery stream can be configured to receive records directly\n *          from providers using <a>PutRecord</a> or <a>PutRecordBatch</a>, or it\n *          can be configured to use an existing Kinesis stream as its source. To specify a Kinesis\n *          data stream as input, set the <code>DeliveryStreamType</code> parameter to\n *             <code>KinesisStreamAsSource</code>, and provide the Kinesis stream Amazon Resource Name\n *          (ARN) and role ARN in the <code>KinesisStreamSourceConfiguration</code>\n *          parameter.</p>\n *          <p>To create a delivery stream with server-side encryption (SSE) enabled, include <a>DeliveryStreamEncryptionConfigurationInput</a> in your request. This is\n *          optional. You can also invoke <a>StartDeliveryStreamEncryption</a> to turn on\n *          SSE for an existing delivery stream that doesn't have SSE enabled.</p>\n *          <p>A delivery stream is configured with a single destination: Amazon S3, Amazon ES,\n *          Amazon Redshift, or Splunk. You must specify only one of the following destination\n *          configuration parameters: <code>ExtendedS3DestinationConfiguration</code>,\n *             <code>S3DestinationConfiguration</code>,\n *             <code>ElasticsearchDestinationConfiguration</code>,\n *             <code>RedshiftDestinationConfiguration</code>, or\n *             <code>SplunkDestinationConfiguration</code>.</p>\n *          <p>When you specify <code>S3DestinationConfiguration</code>, you can also provide the\n *          following optional values: BufferingHints, <code>EncryptionConfiguration</code>, and\n *             <code>CompressionFormat</code>. By default, if no <code>BufferingHints</code> value is\n *          provided, Kinesis Data Firehose buffers data up to 5 MB or for 5 minutes, whichever\n *          condition is satisfied first. <code>BufferingHints</code> is a hint, so there are some\n *          cases where the service cannot adhere to these conditions strictly. For example, record\n *          boundaries might be such that the size is a little over or under the configured buffering\n *          size. By default, no encryption is performed. We strongly recommend that you enable\n *          encryption to ensure secure data storage in Amazon S3.</p>\n *\n *          <p>A few notes about Amazon Redshift as a destination:</p>\n *          <ul>\n *             <li>\n *                <p>An Amazon Redshift destination requires an S3 bucket as intermediate location.\n *                Kinesis Data Firehose first delivers data to Amazon S3 and then uses\n *                   <code>COPY</code> syntax to load data into an Amazon Redshift table. This is\n *                specified in the <code>RedshiftDestinationConfiguration.S3Configuration</code>\n *                parameter.</p>\n *\n *             </li>\n *             <li>\n *                <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be\n *                specified in <code>RedshiftDestinationConfiguration.S3Configuration</code> because\n *                the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't\n *                support these compression formats.</p>\n *             </li>\n *             <li>\n *                <p>We strongly recommend that you use the user name and password you provide\n *                exclusively with Kinesis Data Firehose, and that the permissions for the account are\n *                restricted for Amazon Redshift <code>INSERT</code> permissions.</p>\n *\n *             </li>\n *          </ul>\n *          <p>Kinesis Data Firehose assumes the IAM role that is configured as part of the\n *          destination. The role should allow the Kinesis Data Firehose principal to assume the role,\n *          and the role should have permissions that allow the service to deliver the data. For more\n *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n *             Firehose Access to an Amazon S3 Destination</a> in the <i>Amazon Kinesis Data\n *             Firehose Developer Guide</i>.</p>\n */\n\nvar CreateDeliveryStreamCommand =\n/** @class */\nfunction (_super) {\n  __extends(CreateDeliveryStreamCommand, _super); // Start section: command_properties\n  // End section: command_properties\n\n\n  function CreateDeliveryStreamCommand(input) {\n    var _this = // Start section: command_constructor\n    _super.call(this) || this;\n\n    _this.input = input;\n    return _this; // End section: command_constructor\n  }\n  /**\n   * @internal\n   */\n\n\n  CreateDeliveryStreamCommand.prototype.resolveMiddleware = function (clientStack, configuration, options) {\n    this.middlewareStack.use(getSerdePlugin(configuration, this.serialize, this.deserialize));\n    var stack = clientStack.concat(this.middlewareStack);\n    var logger = configuration.logger;\n    var clientName = \"FirehoseClient\";\n    var commandName = \"CreateDeliveryStreamCommand\";\n    var handlerExecutionContext = {\n      logger: logger,\n      clientName: clientName,\n      commandName: commandName,\n      inputFilterSensitiveLog: CreateDeliveryStreamInput.filterSensitiveLog,\n      outputFilterSensitiveLog: CreateDeliveryStreamOutput.filterSensitiveLog\n    };\n    var requestHandler = configuration.requestHandler;\n    return stack.resolve(function (request) {\n      return requestHandler.handle(request.request, options || {});\n    }, handlerExecutionContext);\n  };\n\n  CreateDeliveryStreamCommand.prototype.serialize = function (input, context) {\n    return serializeAws_json1_1CreateDeliveryStreamCommand(input, context);\n  };\n\n  CreateDeliveryStreamCommand.prototype.deserialize = function (output, context) {\n    return deserializeAws_json1_1CreateDeliveryStreamCommand(output, context);\n  };\n\n  return CreateDeliveryStreamCommand;\n}($Command);\n\nexport { CreateDeliveryStreamCommand };","map":{"version":3,"sources":["../../../commands/CreateDeliveryStreamCommand.ts"],"names":[],"mappings":";AACA,SAAS,yBAAT,EAAoC,0BAApC,QAAsE,oBAAtE;AACA,SACE,iDADF,EAEE,+CAFF,QAGO,0BAHP;AAIA,SAAS,cAAT,QAA+B,2BAA/B;AAEA,SAAS,OAAO,IAAI,QAApB,QAAoC,wBAApC;AAcA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAsEG;;AACH,IAAA,2BAAA;AAAA;AAAA,UAAA,MAAA,EAAA;AAAiD,EAAA,SAAA,CAAA,2BAAA,EAAA,MAAA,CAAA,CAAjD,CAKE;AACA;;;AAEA,WAAA,2BAAA,CAAqB,KAArB,EAA4D;AAA5D,QAAA,KAAA,GACE;AACA,IAAA,MAAA,CAAA,IAAA,CAAA,IAAA,KAAO,IAFT;;AAAqB,IAAA,KAAA,CAAA,KAAA,GAAA,KAAA;iBAAuC,CAG1D;AACD;AAED;;AAEG;;;AACH,EAAA,2BAAA,CAAA,SAAA,CAAA,iBAAA,GAAA,UACE,WADF,EAEE,aAFF,EAGE,OAHF,EAGgC;AAE9B,SAAK,eAAL,CAAqB,GAArB,CAAyB,cAAc,CAAC,aAAD,EAAgB,KAAK,SAArB,EAAgC,KAAK,WAArC,CAAvC;AAEA,QAAM,KAAK,GAAG,WAAW,CAAC,MAAZ,CAAmB,KAAK,eAAxB,CAAd;AAEQ,QAAA,MAAM,GAAK,aAAa,CAAlB,MAAN;AACR,QAAM,UAAU,GAAG,gBAAnB;AACA,QAAM,WAAW,GAAG,6BAApB;AACA,QAAM,uBAAuB,GAA4B;AACvD,MAAA,MAAM,EAAA,MADiD;AAEvD,MAAA,UAAU,EAAA,UAF6C;AAGvD,MAAA,WAAW,EAAA,WAH4C;AAIvD,MAAA,uBAAuB,EAAE,yBAAyB,CAAC,kBAJI;AAKvD,MAAA,wBAAwB,EAAE,0BAA0B,CAAC;AALE,KAAzD;AAOQ,QAAA,cAAc,GAAK,aAAa,CAAlB,cAAd;AACR,WAAO,KAAK,CAAC,OAAN,CACL,UAAC,OAAD,EAAuC;AACrC,aAAA,cAAc,CAAC,MAAf,CAAsB,OAAO,CAAC,OAA9B,EAAwD,OAAO,IAAI,EAAnE,CAAA;AAAsE,KAFnE,EAGL,uBAHK,CAAP;AAKD,GAzBD;;AA2BQ,EAAA,2BAAA,CAAA,SAAA,CAAA,SAAA,GAAR,UAAkB,KAAlB,EAA2D,OAA3D,EAAkF;AAChF,WAAO,+CAA+C,CAAC,KAAD,EAAQ,OAAR,CAAtD;AACD,GAFO;;AAIA,EAAA,2BAAA,CAAA,SAAA,CAAA,WAAA,GAAR,UAAoB,MAApB,EAA4C,OAA5C,EAAmE;AACjE,WAAO,iDAAiD,CAAC,MAAD,EAAS,OAAT,CAAxD;AACD,GAFO;;AAMV,SAAA,2BAAA;AAAC,CAtDD,CAAiD,QAAjD,CAAA","sourceRoot":"","sourcesContent":["import { __extends } from \"tslib\";\nimport { CreateDeliveryStreamInput, CreateDeliveryStreamOutput } from \"../models/models_0\";\nimport { deserializeAws_json1_1CreateDeliveryStreamCommand, serializeAws_json1_1CreateDeliveryStreamCommand, } from \"../protocols/Aws_json1_1\";\nimport { getSerdePlugin } from \"@aws-sdk/middleware-serde\";\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\n/**\n * <p>Creates a Kinesis Data Firehose delivery stream.</p>\n *\n *          <p>By default, you can create up to 50 delivery streams per AWS Region.</p>\n *          <p>This is an asynchronous operation that immediately returns. The initial status of the\n *          delivery stream is <code>CREATING</code>. After the delivery stream is created, its status\n *          is <code>ACTIVE</code> and it now accepts data. If the delivery stream creation fails, the\n *          status transitions to <code>CREATING_FAILED</code>. Attempts to send data to a delivery\n *          stream that is not in the <code>ACTIVE</code> state cause an exception. To check the state\n *          of a delivery stream, use <a>DescribeDeliveryStream</a>.</p>\n *          <p>If the status of a delivery stream is <code>CREATING_FAILED</code>, this status\n *          doesn't change, and you can't invoke <code>CreateDeliveryStream</code> again on it.\n *          However, you can invoke the <a>DeleteDeliveryStream</a> operation to delete\n *          it.</p>\n *          <p>A Kinesis Data Firehose delivery stream can be configured to receive records directly\n *          from providers using <a>PutRecord</a> or <a>PutRecordBatch</a>, or it\n *          can be configured to use an existing Kinesis stream as its source. To specify a Kinesis\n *          data stream as input, set the <code>DeliveryStreamType</code> parameter to\n *             <code>KinesisStreamAsSource</code>, and provide the Kinesis stream Amazon Resource Name\n *          (ARN) and role ARN in the <code>KinesisStreamSourceConfiguration</code>\n *          parameter.</p>\n *          <p>To create a delivery stream with server-side encryption (SSE) enabled, include <a>DeliveryStreamEncryptionConfigurationInput</a> in your request. This is\n *          optional. You can also invoke <a>StartDeliveryStreamEncryption</a> to turn on\n *          SSE for an existing delivery stream that doesn't have SSE enabled.</p>\n *          <p>A delivery stream is configured with a single destination: Amazon S3, Amazon ES,\n *          Amazon Redshift, or Splunk. You must specify only one of the following destination\n *          configuration parameters: <code>ExtendedS3DestinationConfiguration</code>,\n *             <code>S3DestinationConfiguration</code>,\n *             <code>ElasticsearchDestinationConfiguration</code>,\n *             <code>RedshiftDestinationConfiguration</code>, or\n *             <code>SplunkDestinationConfiguration</code>.</p>\n *          <p>When you specify <code>S3DestinationConfiguration</code>, you can also provide the\n *          following optional values: BufferingHints, <code>EncryptionConfiguration</code>, and\n *             <code>CompressionFormat</code>. By default, if no <code>BufferingHints</code> value is\n *          provided, Kinesis Data Firehose buffers data up to 5 MB or for 5 minutes, whichever\n *          condition is satisfied first. <code>BufferingHints</code> is a hint, so there are some\n *          cases where the service cannot adhere to these conditions strictly. For example, record\n *          boundaries might be such that the size is a little over or under the configured buffering\n *          size. By default, no encryption is performed. We strongly recommend that you enable\n *          encryption to ensure secure data storage in Amazon S3.</p>\n *\n *          <p>A few notes about Amazon Redshift as a destination:</p>\n *          <ul>\n *             <li>\n *                <p>An Amazon Redshift destination requires an S3 bucket as intermediate location.\n *                Kinesis Data Firehose first delivers data to Amazon S3 and then uses\n *                   <code>COPY</code> syntax to load data into an Amazon Redshift table. This is\n *                specified in the <code>RedshiftDestinationConfiguration.S3Configuration</code>\n *                parameter.</p>\n *\n *             </li>\n *             <li>\n *                <p>The compression formats <code>SNAPPY</code> or <code>ZIP</code> cannot be\n *                specified in <code>RedshiftDestinationConfiguration.S3Configuration</code> because\n *                the Amazon Redshift <code>COPY</code> operation that reads from the S3 bucket doesn't\n *                support these compression formats.</p>\n *             </li>\n *             <li>\n *                <p>We strongly recommend that you use the user name and password you provide\n *                exclusively with Kinesis Data Firehose, and that the permissions for the account are\n *                restricted for Amazon Redshift <code>INSERT</code> permissions.</p>\n *\n *             </li>\n *          </ul>\n *          <p>Kinesis Data Firehose assumes the IAM role that is configured as part of the\n *          destination. The role should allow the Kinesis Data Firehose principal to assume the role,\n *          and the role should have permissions that allow the service to deliver the data. For more\n *          information, see <a href=\"https://docs.aws.amazon.com/firehose/latest/dev/controlling-access.html#using-iam-s3\">Grant Kinesis Data\n *             Firehose Access to an Amazon S3 Destination</a> in the <i>Amazon Kinesis Data\n *             Firehose Developer Guide</i>.</p>\n */\nvar CreateDeliveryStreamCommand = /** @class */ (function (_super) {\n    __extends(CreateDeliveryStreamCommand, _super);\n    // Start section: command_properties\n    // End section: command_properties\n    function CreateDeliveryStreamCommand(input) {\n        var _this = \n        // Start section: command_constructor\n        _super.call(this) || this;\n        _this.input = input;\n        return _this;\n        // End section: command_constructor\n    }\n    /**\n     * @internal\n     */\n    CreateDeliveryStreamCommand.prototype.resolveMiddleware = function (clientStack, configuration, options) {\n        this.middlewareStack.use(getSerdePlugin(configuration, this.serialize, this.deserialize));\n        var stack = clientStack.concat(this.middlewareStack);\n        var logger = configuration.logger;\n        var clientName = \"FirehoseClient\";\n        var commandName = \"CreateDeliveryStreamCommand\";\n        var handlerExecutionContext = {\n            logger: logger,\n            clientName: clientName,\n            commandName: commandName,\n            inputFilterSensitiveLog: CreateDeliveryStreamInput.filterSensitiveLog,\n            outputFilterSensitiveLog: CreateDeliveryStreamOutput.filterSensitiveLog,\n        };\n        var requestHandler = configuration.requestHandler;\n        return stack.resolve(function (request) {\n            return requestHandler.handle(request.request, options || {});\n        }, handlerExecutionContext);\n    };\n    CreateDeliveryStreamCommand.prototype.serialize = function (input, context) {\n        return serializeAws_json1_1CreateDeliveryStreamCommand(input, context);\n    };\n    CreateDeliveryStreamCommand.prototype.deserialize = function (output, context) {\n        return deserializeAws_json1_1CreateDeliveryStreamCommand(output, context);\n    };\n    return CreateDeliveryStreamCommand;\n}($Command));\nexport { CreateDeliveryStreamCommand };\n//# sourceMappingURL=CreateDeliveryStreamCommand.js.map"]},"metadata":{},"sourceType":"module"}